# 添加频道指南

本指南将详细介绍如何在本项目中添加新的频道(channel)支持。

## 频道基本结构

一个普通频道应该包含以下属性：
- `furl`：搜索结果的URL
- `source`：频道名称
- `url`：视频/图片的URL
- `title`：标题
- `savetitle`：保存时的标题
- `author`：作者
- `updatedAt`：更新时间
- `numViews`：观看次数
- `dpath`：下载路径

## 添加频道步骤

### 1. 修改 src/config/Init_Settings.py

在 `CHANNELS_CONFIG` 字典中添加新频道的配置：

```python
CHANNELS_CONFIG = {
    # 现有频道...
    "新频道名称": {
        "hostname_key": "新频道_Hostname",
        "download_path_key": "新频道_Download_Path",
        "default_hostname": "https://www.新频道域名.com",
        "default_download_path": os.path.join(os.path.expanduser("~"), "新频道_Downloads")
    }
}
```

### 2. 修改 src/core/Custom_Struc.py

创建新频道的视频结构体，继承或实现以下基本属性：

```python
class stru_新频道_video:
    def __init__(self, data: dict):
        # 频道必须属性
        self.furl: str = data.get("furl", "").strip()
        self.source: str = "新频道名称"
        self.url: str = data.get("url", "").strip()
        self.title: str = re.sub(r'[\\/*?:"<>|]', "_", data.get("title", "").strip())
        self.savetitle: str = self.title  # 根据需要格式化保存标题
        self.author: str = data.get("author", "").strip()
        self.updatedAt: str = data.get("updatedAt", "").strip()
        self.numViews: int = data.get("numViews", 0)
        self.dpath: str = sm.settings.get("新频道_Download_Path", DEFAULT_SETTINGS["新频道_Download_Path"])
        self.dpath = os.path.join(self.dpath, self.author)  # 可以根据需要调整路径结构

    def get(self, key: str, default: Any = None) -> Any:
        return self.__dict__.get(key, default)
    
    # 可以添加额外的方法来适配该频道的特殊需求
```

### 3. 修改 src/core/Search_Engine.py

实现新频道的搜索方法，并在 `register_search_channels` 函数中注册新频道：

#### 添加搜索方法

```python
@staticmethod
def 新频道_search_video(keyword: str) -> list[stru_新频道_video]:
    try:
        # 实现搜索逻辑
        # 1. 发送请求获取搜索结果
        # 2. 解析HTML或JSON获取视频信息
        # 3. 创建 stru_新频道_video 对象并添加到列表
        # 4. 返回视频列表
        pass
    except Exception as e:
        logger.error(f"处理新频道搜索结果时发生未知错误: {e}")
        return []
```

#### 注册新频道

在 `register_search_channels` 函数末尾添加：

```python
# 注册新频道
新频道_channel = Channel(
    name="新频道名称",
    hostname_key="新频道_Hostname",
    download_path_key="新频道_Download_Path",
    search_method=Search_Engine.新频道_search_video,
    download_methods={
        "default": Download_Engine.新频道_download_video
        # 可以添加其他下载方法，如 pic、video 等
    },
    video_struc=stru_新频道_video,
)
channel_manager.register_channel(新频道_channel)
```

### 4. 修改 src/core/Download_Engine.py

实现新频道的下载方法：

```python
@staticmethod
def 新频道_download_video(video: stru_新频道_video) -> bool:
    try:
        # 实现下载逻辑
        # 1. 获取视频下载链接
        # 2. 下载视频到指定路径
        # 3. 返回下载结果
        pass
    except Exception as e:
        logger.error(f"下载新频道视频失败: {video.savetitle}, 错误: {e}")
        return False
```

### 5. 修改 src/ui/UI.py

#### 5.1 Window_Settings 类的动态设置项

根据实际代码，`Window_Settings` 类已经采用了动态创建频道设置项的方式，会自动从 `CHANNELS_CONFIG` 中读取所有频道配置并创建对应的设置项。因此，当你在 `CHANNELS_CONFIG` 中添加新频道后，设置界面会自动显示该频道的设置项，无需手动修改 `create_widgets`、`fill_entry` 和 `on_close` 方法。

#### 5.2 在 Win_Main 类中添加搜索源选项

在 `create_widgets` 方法中，搜索源下拉框会自动从 `channel_manager` 中获取所有已注册的频道，无需手动添加。

#### 5.3 在 Window_Favor 类中添加收藏夹支持

根据实际代码，`Window_Favor` 类已经采用了动态处理收藏夹的方式，会自动从 `channel_manager` 中获取所有已注册的频道，并为每个频道创建对应的标签页和作者列表。因此，当你在 `CHANNELS_CONFIG` 中添加新频道并注册后，收藏夹界面会自动显示该频道的标签页，无需手动修改 `Window_Favor` 类的代码。

`Window_Favor` 类的主要动态处理逻辑包括：

1. 从 `channel_manager` 获取所有已注册频道
2. 为每个频道自动创建标签页和对应的作者列表
3. `edit_favor` 方法会动态为每个频道创建编辑区域
4. `save_favor` 函数使用字典来保存所有频道的作者列表

因此，添加新频道后，收藏夹支持会自动生效，无需额外修改代码。

### 6. 测试新频道

添加完成后，运行程序测试新频道的搜索和下载功能是否正常工作。

## 频道实现示例

以下是一个简化的频道实现示例，供参考：

### src/config/Init_Settings.py

```python
CHANNELS_CONFIG = {
    # 现有频道...
    "Example": {
        "hostname_key": "Example_Hostname",
        "download_path_key": "Example_Download_Path",
        "default_hostname": "https://www.example.com",
        "default_download_path": os.path.join(os.path.expanduser("~"), "Example_Downloads")
    }
}
```

### src/core/Custom_Struc.py

```python
class stru_example_video:
    def __init__(self, data: dict):
        self.furl: str = data.get("furl", "").strip()
        self.source: str = "Example"
        self.url: str = data.get("url", "").strip()
        self.title: str = re.sub(r'[\\/*?:"<>|]', "_", data.get("title", "").strip())
        self.updatedAt: str = data.get("updatedAt", "").strip()
        self.savetitle: str = f"[{self.updatedAt}] {self.title}"
        self.savetitle = re.sub(r'[\\/*?:"<>|]', "_", self.savetitle)
        self.author: str = data.get("author", "").strip()
        self.numViews: int = data.get("numViews", 0)
        self.dpath: str = sm.settings.get("Example_Download_Path", DEFAULT_SETTINGS["Example_Download_Path"])
        self.dpath = os.path.join(self.dpath, self.author)

    def get(self, key: str, default: Any = None) -> Any:
        return self.__dict__.get(key, default)
```

### src/core/Search_Engine.py

```python
@staticmethod
def example_search_video(keyword: str) -> list[stru_example_video]:
    try:
        base_url = sm.settings.get("Example_Hostname", DEFAULT_SETTINGS["Example_Hostname"])
        search_url = f"{base_url}/search?q={keyword}"
        
        response = scraper_manager.get_cloud_scraper().get_instance().get(
            url=search_url, timeout=5, 
            proxies=PROXIES, 
            verify=sm.settings.get("Check_Cert", DEFAULT_SETTINGS["Check_Cert"])
        )
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        video_list = []
        
        # 解析视频列表
        for video_item in soup.find_all("div", class_="video-item"):
            # 提取视频信息
            title = video_item.find("h3").text.strip()
            url = video_item.find("a", href=True)["href"]
            author = video_item.find("span", class_="author").text.strip()
            updatedAt = video_item.find("span", class_="date").text.strip()
            numViews = int(video_item.find("span", class_="views").text.strip())
            
            # 创建视频对象
            video = stru_example_video({
                "furl": search_url,
                "url": f"{base_url}{url}",
                "title": title,
                "author": author,
                "updatedAt": updatedAt,
                "numViews": numViews
            })
            video_list.append(video)
        
        return video_list
    except Exception as e:
        logger.error(f"处理Example搜索结果时发生未知错误: {e}")
        return []
```

### src/core/Download_Engine.py

```python
@staticmethod
def example_download_video(video: stru_example_video) -> bool:
    try:
        # 获取视频下载链接
        response = scraper_manager.get_cloud_scraper().get_instance().get(
            url=video.url, timeout=5,
            verify=sm.settings.get("Check_Cert", DEFAULT_SETTINGS["Check_Cert"])
        )
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        video_tag = soup.find("video")
        if not video_tag:
            logger.error("未找到视频标签")
            return False
        
        video_url = video_tag.get("src")
        if not video_url:
            logger.error("未找到视频URL")
            return False
        
        # 下载视频
        save_path = os.path.join(video.dpath, f"{video.savetitle}.mp4")
        
        headers = DEFAULT_HEADERS.copy()
        headers["Referer"] = video.url
        
        opts = {
            "format": "bestvideo+bestaudio/best",
            "outtmpl": save_path,
            "nocheckcertificate": not sm.settings.get("Check_Cert", DEFAULT_SETTINGS["Check_Cert"]),
            "useragent": headers["User-Agent"],
            "referer": headers["Referer"],
            "http_headers": headers,
            "quiet": True,
            "no_warnings": True,
            "logtostderr": True,
        }
        with yt_dlp.YoutubeDL(opts) as ydl:
            ydl.download([video_url])
        
        logger.info(f"下载完成: {video.savetitle}")
        return True
    except Exception as e:
        logger.error(f"下载Example视频失败: {video.savetitle}, 错误: {e}")
        return False
```

## 注意事项

1. **错误处理**：在实现搜索和下载方法时，一定要添加完善的错误处理，确保程序不会崩溃。
2. **日志记录**：使用 `logger` 对象记录关键操作和错误信息，便于调试和监控。
3. **设置管理**：通过 `sm.settings` 获取频道的配置信息，如主机名、下载路径等。
4. **缓存管理**：搜索结果会自动缓存，无需手动处理。
5. **进度跟踪**：对于大文件下载，建议使用 `DownloadProgressTracker` 显示下载进度。
6. **代理支持**：根据需要使用 `PROXIES` 变量设置代理。
7. **证书验证**：使用 `sm.settings.get("Check_Cert", DEFAULT_SETTINGS["Check_Cert"])` 控制是否验证SSL证书。

## 具体爬虫方式示例

在实现搜索和下载方法时，建议使用项目中提供的 `scraper_manager` 对象（从 `utils.CScraper` 模块导入），它已经配置好了Cloudflare绕过和浏览器模拟。

### 1. GET请求示例

```python
from ..utils.CScraper import scraper_manager
from ..config.Init_Settings import PROXIES, DEFAULT_SETTINGS
from ..config.Settings_Manager import sm

# 发送GET请求
response = scraper_manager.get_cloud_scraper().get_instance().get(
    url="需要获取的页面URL",
    headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"},
    timeout=5,  # 超时时间
    proxies=PROXIES,  # 使用代理
    verify=sm.settings.get("Check_Cert", DEFAULT_SETTINGS["Check_Cert"])  # 控制是否验证SSL证书
)

# 检查响应状态
response.raise_for_status()  # 如果状态码不是200-399，抛出异常

# 获取响应内容
soup = BeautifulSoup(response.text, "html.parser")
```

### 2. POST请求示例

```python
# 发送POST请求
response = scraper_manager.get_cloud_scraper().get_instance().post(
    url="需要发送POST请求的URL",
    data={"key1": "value1", "key2": "value2"},  # POST数据
    headers={"Content-Type": "application/x-www-form-urlencoded"},
    timeout=5,
    proxies=PROXIES,
    verify=sm.settings.get("Check_Cert", DEFAULT_SETTINGS["Check_Cert"])
)

response.raise_for_status()
```

### 3. 处理JSON响应

```python
# 发送请求获取JSON数据
response = scraper_manager.get_cloud_scraper().get_instance().get(
    url="API URL",
    headers={"Accept": "application/json"},
    timeout=5,
    proxies=PROXIES,
    verify=sm.settings.get("Check_Cert", DEFAULT_SETTINGS["Check_Cert"])
)

response.raise_for_status()
data = response.json()  # 解析JSON响应
```

### 4. 流式下载示例

```python
# 流式下载大文件
response = scraper_manager.get_cloud_scraper().get_instance().get(
    url="大文件URL",
    timeout=30,
    proxies=PROXIES,
    verify=sm.settings.get("Check_Cert", DEFAULT_SETTINGS["Check_Cert"]),
    stream=True  # 启用流式下载
)

response.raise_for_status()

# 获取文件总大小
total_size = int(response.headers.get('content-length', 0))

# 创建进度跟踪器
tracker = DownloadProgressTracker("文件名")
tracker.total_size = total_size

downloaded = 0
with open("保存路径", "wb") as f:
    for chunk in response.iter_content(chunk_size=1024):
        if chunk:
            f.write(chunk)
            f.flush()
            downloaded += len(chunk)
            tracker.update(downloaded)

tracker.finish()
```

### 5. 使用yt-dlp下载示例

```python
import yt_dlp
from ..config.Init_Settings import DEFAULT_HEADERS, DEFAULT_SETTINGS
from ..config.Settings_Manager import sm

# 使用yt-dlp下载视频
ydl_opts: dict = {
    "format": "bestvideo+bestaudio/best",
    "outtmpl": "保存路径",
    "nocheckcertificate": not sm.settings.get("Check_Cert", DEFAULT_SETTINGS["Check_Cert"]),
    "useragent": DEFAULT_HEADERS["User-Agent"],
    "referer": "引用URL",
    "http_headers": DEFAULT_HEADERS,
    "quiet": True,
    "no_warnings": True,
    "logtostderr": True,
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download(["视频URL"])
```

## 扩展功能

除了基本的搜索和下载功能，还可以为新频道添加以下扩展功能：

1. **多类型支持**：支持视频、图片等多种内容类型。
2. **自定义下载方法**：根据不同的内容类型实现不同的下载逻辑。
3. **登录支持**：如果频道需要登录才能访问，可以参考 `Iwara_Login.py` 实现登录功能。
4. **高级搜索**：支持按分类、日期等条件进行搜索。
5. **批量下载**：支持批量下载多个视频或图片。

## 测试流程

1. **单元测试**：测试搜索方法是否能正确返回视频列表。
2. **集成测试**：测试完整的搜索-下载流程是否正常工作。
3. **边界测试**：测试搜索关键词为空、网络异常等边界情况。
4. **性能测试**：测试搜索和下载的性能表现。

通过以上步骤，您可以成功添加新的频道支持。祝您添加愉快！